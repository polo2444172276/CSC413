{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "s9IS9B9-yUU5",
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "dCae1mOUlZrC",
        "TSDTbsydlaGI",
        "vYPae08Io1Fi",
        "xq7nhsEio1w-",
        "9tcpUFKqo2Oi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "98c1057d-6361-4964-f714-bfa04e518991"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Collecting pillow>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6d/b719ae8e21660a6a962636896dc4b7d657ef451a3ab941516401846ac5cb/Pillow-8.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 17.7MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-8.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03/Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 8.1.2\n",
            "    Uninstalling Pillow-8.1.2:\n",
            "      Successfully uninstalled Pillow-8.1.2\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "db1dc3ae-71b2-4321-85d7-26248b982105"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(MyLSTMCell, self).__init__()\r\n",
        "\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        # ------------\r\n",
        "        # FILL THIS IN\r\n",
        "        # ------------\r\n",
        "        #Linear: R^{dim_in} -> R^{dim_out}\r\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, h_prev, c_prev):\r\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\r\n",
        "\r\n",
        "        Arguments\r\n",
        "            x: batch_size x input_size\r\n",
        "            h_prev: batch_size x hidden_size\r\n",
        "            c_prev: batch_size x hidden_size\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            h_new: batch_size x hidden_size\r\n",
        "            c_new: batch_size x hidden_size\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # ------------\r\n",
        "        # FILL THIS IN\r\n",
        "        # ------------\r\n",
        "        sigmoid = nn.Sigmoid()\r\n",
        "        tanh = nn.Tanh()\r\n",
        "        i = sigmoid(self.Wii(x) + self.Whi(h_prev))\r\n",
        "        f = sigmoid(self.Wif(x) + self.Whf(h_prev))\r\n",
        "        g = tanh(self.Wig(x) + self.Whg(h_prev))\r\n",
        "        o = sigmoid(self.Wio(x) + self.Who(h_prev))\r\n",
        "        c_new = f * c_prev + i * g\r\n",
        "        h_new = o * tanh(c_new)\r\n",
        "        return h_new, c_new"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f503b7fe-3b9c-4fce-b5b7-9ef67da50f59"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "rnn_args_s = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True,\r\n",
        "              'nepochs':50,\r\n",
        "              'checkpoint_dir':\"checkpoints\",\r\n",
        "              'learning_rate':0.005,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'early_stopping_patience':20,\r\n",
        "              'batch_size':64,\r\n",
        "              'hidden_size':32,\r\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\r\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\r\n",
        "              'attention_type': '',  # options: additive / scaled_dot\r\n",
        "}\r\n",
        "rnn_args_s.update(args_dict)\r\n",
        "\r\n",
        "print_opts(rnn_args_s)\r\n",
        "\r\n",
        "import time\r\n",
        "t1 = time.process_time()\r\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\r\n",
        "print(f\"training time: {time.process_time() - t1} \")\r\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.363 | Val loss: 1.981 | Gen: inway ay-ay insay-ingay-inway inway insay-inway\n",
            "Epoch:   1 | Train loss: 1.823 | Val loss: 1.743 | Gen: areway areway ongiongay-ingay isay ongway-inway\n",
            "Epoch:   2 | Train loss: 1.609 | Val loss: 1.626 | Gen: ereway areway ongingsay-ingway isay ongerway\n",
            "Epoch:   3 | Train loss: 1.471 | Val loss: 1.536 | Gen: erway arway onsionssay-ingway isway ousedway\n",
            "Epoch:   4 | Train loss: 1.361 | Val loss: 1.451 | Gen: etay arway onsionsedsay-ay isway ousedway\n",
            "Epoch:   5 | Train loss: 1.257 | Val loss: 1.395 | Gen: etay arway ollasionssay-away isway ousedway\n",
            "Epoch:   6 | Train loss: 1.174 | Val loss: 1.337 | Gen: etay arway onglinsay-ingway isway oustedway\n",
            "Epoch:   7 | Train loss: 1.091 | Val loss: 1.249 | Gen: etay arway oncinglay-ingway isway ondedsway\n",
            "Epoch:   8 | Train loss: 1.013 | Val loss: 1.191 | Gen: etay arway ollway-ingssay-awlay isway ousedway\n",
            "Epoch:   9 | Train loss: 0.956 | Val loss: 1.192 | Gen: etay arway ollinsay-ingenway isway ourdedway\n",
            "Epoch:  10 | Train loss: 0.909 | Val loss: 1.129 | Gen: ethay arway ollinsingstray isway oudeddway\n",
            "Epoch:  11 | Train loss: 0.856 | Val loss: 1.132 | Gen: ethay arway ontionsingstay isway ourdingway\n",
            "Epoch:  12 | Train loss: 0.818 | Val loss: 1.084 | Gen: ehay arway ollansionceray isway ourdednay\n",
            "Epoch:  13 | Train loss: 0.762 | Val loss: 1.039 | Gen: ehay arway ontiongsingway isway ourdedway\n",
            "Epoch:  14 | Train loss: 0.721 | Val loss: 1.153 | Gen: ehay arway onationsingsway isway ourgstay\n",
            "Epoch:  15 | Train loss: 0.706 | Val loss: 0.981 | Gen: ehay arway ontionsingray isway ourdingway\n",
            "Epoch:  16 | Train loss: 0.645 | Val loss: 0.997 | Gen: ehay arway ontiongingsray isway ourdingway\n",
            "Epoch:  17 | Train loss: 0.639 | Val loss: 0.986 | Gen: ehay arway outiongscingway isway ourdingway\n",
            "Epoch:  18 | Train loss: 0.599 | Val loss: 0.971 | Gen: ehay arway olusingtringway isway ourdingway\n",
            "Epoch:  19 | Train loss: 0.572 | Val loss: 0.948 | Gen: ethay arway ontiongingsray isway ourdingway\n",
            "Epoch:  20 | Train loss: 0.552 | Val loss: 0.926 | Gen: ethay arway ontioningsingway isway orudingway\n",
            "Epoch:  21 | Train loss: 0.516 | Val loss: 0.918 | Gen: ethay arway ontiongingssway isway ouringway\n",
            "Epoch:  22 | Train loss: 0.494 | Val loss: 0.873 | Gen: ehay arway ontiongidingway isway ourdingway\n",
            "Epoch:  23 | Train loss: 0.477 | Val loss: 0.950 | Gen: ethay arway ontingsingcay isway ourdingway\n",
            "Epoch:  24 | Train loss: 0.484 | Val loss: 0.905 | Gen: ethay arway ontingingsricay isway ouringway\n",
            "Epoch:  25 | Train loss: 0.461 | Val loss: 0.889 | Gen: ethay ariway ontioningsricay isway orusingway\n",
            "Epoch:  26 | Train loss: 0.444 | Val loss: 0.894 | Gen: ethay ariway ontiondingciblay isway orudingway\n",
            "Epoch:  27 | Train loss: 0.437 | Val loss: 0.836 | Gen: ethay ariway ontiondigciblay isway oruringway\n",
            "Epoch:  28 | Train loss: 0.396 | Val loss: 0.790 | Gen: ethay arway ontionigdincay isway orudingway\n",
            "Epoch:  29 | Train loss: 0.393 | Val loss: 0.841 | Gen: ethay ariway ontionigsdray isway ouringway\n",
            "Epoch:  30 | Train loss: 0.382 | Val loss: 0.861 | Gen: ethay arway ontionigingcay isway oringray\n",
            "Epoch:  31 | Train loss: 0.385 | Val loss: 0.832 | Gen: ethay ariway ontiondiciglway isway oruringway\n",
            "Epoch:  32 | Train loss: 0.373 | Val loss: 0.822 | Gen: ethay ariway ontionigdingway isway oringray\n",
            "Epoch:  33 | Train loss: 0.351 | Val loss: 0.840 | Gen: ethay ariway ontionigscingway isway orudtingway\n",
            "Epoch:  34 | Train loss: 0.334 | Val loss: 0.807 | Gen: ethay ariway ontiouningrilay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.316 | Val loss: 0.793 | Gen: ethay ariway ontionigdincay isway orudinway\n",
            "Epoch:  36 | Train loss: 0.304 | Val loss: 0.815 | Gen: ethay ariway onationdicicay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.297 | Val loss: 0.858 | Gen: ethay ariway ontinighinglyray isway orkingway\n",
            "Epoch:  38 | Train loss: 0.296 | Val loss: 0.939 | Gen: ethay ariway onationidincay isway oringray\n",
            "Epoch:  39 | Train loss: 0.299 | Val loss: 0.903 | Gen: ethay ariway ontiounilicinay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.314 | Val loss: 0.871 | Gen: ethay ariway ontionighrinlay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.309 | Val loss: 0.895 | Gen: ethay airway ontiactingdray isway orkingway\n",
            "Epoch:  42 | Train loss: 0.297 | Val loss: 0.822 | Gen: ethay arway onationdicicay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.271 | Val loss: 0.815 | Gen: ethay ariway onationdicityway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.259 | Val loss: 0.810 | Gen: ethay ariway ontiouniditycay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.246 | Val loss: 0.827 | Gen: ethay ariway ontinighdrlay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.242 | Val loss: 0.858 | Gen: ethay ariway onditingctigday isway orkingway\n",
            "Epoch:  47 | Train loss: 0.234 | Val loss: 0.788 | Gen: ethay arway ontinigdringway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.226 | Val loss: 0.797 | Gen: ethay ariway onationdicicay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.220 | Val loss: 0.922 | Gen: ethay ariway onditingtringay isway orkingway\n",
            "Obtained lowest validation loss of: 0.7883238159120083\n",
            "training time: 177.94199897 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditingtringay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \r\n",
        "\r\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8481486c-ff68-4d3c-8f2d-dcfc1e1620e7"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "\n",
        "import time\n",
        "t1 = time.process_time() \n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "print(f\"training_time: {time.process_time() - t1}\")\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.348 | Val loss: 2.077 | Gen: ontay-ay ay-ay-ay ontay-ontay-ay-ay onay-ay ontay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.906 | Val loss: 1.930 | Gen: ontay ay-atay-ay ootay-ontay-atay-ay- ay-ay-ay-ay-ay-ay-ay otay-intay-ay-ay-ay-\n",
            "Epoch:   2 | Train loss: 1.731 | Val loss: 1.776 | Gen: eray-ay ay-away otay-ontatingay intay otay-intay-ay-ay-ay-\n",
            "Epoch:   3 | Train loss: 1.573 | Val loss: 1.671 | Gen: eray-ay away-ay oontay-intay-inday inway ontay-inday\n",
            "Epoch:   4 | Train loss: 1.450 | Val loss: 1.570 | Gen: eray-ay away-ay oontay-ininsay inway ontay-awlay\n",
            "Epoch:   5 | Train loss: 1.359 | Val loss: 1.536 | Gen: etay-ay away-ay oontay-ininday-awlay inway ontay-inway\n",
            "Epoch:   6 | Train loss: 1.293 | Val loss: 1.475 | Gen: etay-awlay away-ay onionatinday-awlay iway onway-inway-ay\n",
            "Epoch:   7 | Train loss: 1.208 | Val loss: 1.404 | Gen: ethay away-ay oonalionay-inway iway ontay-inway\n",
            "Epoch:   8 | Train loss: 1.137 | Val loss: 1.346 | Gen: etay away-aycay oontay-inionday iway ontay-indway\n",
            "Epoch:   9 | Train loss: 1.066 | Val loss: 1.285 | Gen: etay-ay away-aycay oonabionday-idnay iway ontay-indway\n",
            "Epoch:  10 | Train loss: 0.987 | Val loss: 1.237 | Gen: ethay airyway oonay-idincay-oday iway ordingay\n",
            "Epoch:  11 | Train loss: 0.926 | Val loss: 1.210 | Gen: etay aray-ay oonidincalidyday isway onsionday\n",
            "Epoch:  12 | Train loss: 0.885 | Val loss: 1.162 | Gen: ethay airway ooncinidingay iway oringsay\n",
            "Epoch:  13 | Train loss: 0.841 | Val loss: 1.123 | Gen: ethay airyway oongioncabindway isway ousiveday\n",
            "Epoch:  14 | Train loss: 0.798 | Val loss: 1.081 | Gen: ethay airway oonaidgay-impay isway orgingray\n",
            "Epoch:  15 | Train loss: 0.766 | Val loss: 1.132 | Gen: ethay airay oonaigingpray isway orgingray\n",
            "Epoch:  16 | Train loss: 0.734 | Val loss: 1.062 | Gen: etway airway oingiodincay isway orgingray\n",
            "Epoch:  17 | Train loss: 0.701 | Val loss: 1.029 | Gen: ethay airway oongainidcay isway orgingway\n",
            "Epoch:  18 | Train loss: 0.672 | Val loss: 1.042 | Gen: ethay airway oingicingecay isway orgingray\n",
            "Epoch:  19 | Train loss: 0.653 | Val loss: 1.011 | Gen: etway airway oadiningatoday isway orgingray\n",
            "Epoch:  20 | Train loss: 0.626 | Val loss: 0.963 | Gen: ethay airway oingincay-ioncay isway orgingsay\n",
            "Epoch:  21 | Train loss: 0.606 | Val loss: 0.956 | Gen: ethay airway onicingiday-ocay isway orgingsay\n",
            "Epoch:  22 | Train loss: 0.596 | Val loss: 0.966 | Gen: ethay airway ondinicationway isway orgingsay\n",
            "Epoch:  23 | Train loss: 0.578 | Val loss: 0.884 | Gen: ethay airway oningiompablyway isway orgingsray\n",
            "Epoch:  24 | Train loss: 0.549 | Val loss: 0.910 | Gen: ethay airway onicingay-odecay isway orgingsay\n",
            "Epoch:  25 | Train loss: 0.544 | Val loss: 0.897 | Gen: etway airway oniniodgachay isway orgingsay\n",
            "Epoch:  26 | Train loss: 0.524 | Val loss: 0.875 | Gen: etway airway odinginticay isway orgingsay\n",
            "Epoch:  27 | Train loss: 0.515 | Val loss: 0.858 | Gen: etway airway oadingintioncay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.512 | Val loss: 0.855 | Gen: ethay airway ondinitioglay isway orgisgray\n",
            "Epoch:  29 | Train loss: 0.497 | Val loss: 0.870 | Gen: etway iway odinicongay-acycay isway orgiskybay\n",
            "Epoch:  30 | Train loss: 0.463 | Val loss: 0.827 | Gen: etway airway ondiniopheday isway orkingway\n",
            "Epoch:  31 | Train loss: 0.448 | Val loss: 0.840 | Gen: etway airway ondiingpay-ocay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.443 | Val loss: 0.854 | Gen: etway airway ondiongicanchay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.436 | Val loss: 0.856 | Gen: etway airway ondiicgay-oncray isway orkingway\n",
            "Epoch:  34 | Train loss: 0.422 | Val loss: 0.833 | Gen: etway airway ondinitiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.414 | Val loss: 0.894 | Gen: ethay airway ondiinchilivay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.422 | Val loss: 0.924 | Gen: etway iraway ondiicgay-oncay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.441 | Val loss: 0.831 | Gen: etway airway ondiingpay-oncway isway orginscay\n",
            "Epoch:  38 | Train loss: 0.412 | Val loss: 0.831 | Gen: ethay airway ondiingpostway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.389 | Val loss: 0.780 | Gen: etway airway ondiingpontway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.369 | Val loss: 0.777 | Gen: ethay airway ondiingpay-ocay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.355 | Val loss: 0.755 | Gen: ethay airway ondiingpostway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.345 | Val loss: 0.771 | Gen: ethay airway ondiingpay-ocaray isway orkingway\n",
            "Epoch:  43 | Train loss: 0.345 | Val loss: 0.778 | Gen: ethay airway ondiingpay-ocay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.351 | Val loss: 0.770 | Gen: etway airway ondiicationgyway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.380 | Val loss: 0.871 | Gen: ethay airway ondiingionchay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.391 | Val loss: 0.803 | Gen: ethay airway ondiingpatownay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.356 | Val loss: 0.752 | Gen: etway airway ondiingpablyway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.328 | Val loss: 0.750 | Gen: ethay airway ondiingpay-ofray isway orkingway\n",
            "Epoch:  49 | Train loss: 0.312 | Val loss: 0.721 | Gen: etway airway ondiingpay-ocaysay isway orkingway\n",
            "Obtained lowest validation loss of: 0.7212318416755155\n",
            "training_time: 217.426437599\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tetway airway ondiingpay-ocaysay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1zP_da_G5qfe",
        "outputId": "f0a566d6-3def-41c1-b872-7594001f1a6f"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':64,\n",
        "              'hidden_size':64,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "\n",
        "import time\n",
        "t1 = time.process_time()\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "print(f\"training time: {time.process_time() - t1} \")\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.089 | Val loss: 1.820 | Gen: eray ationsay onsay-onsay-ingay isay onsay-onsay\n",
            "Epoch:   1 | Train loss: 1.577 | Val loss: 1.574 | Gen: eay arsay onsay-ingsay-ingsay issay onseray-ay-ingway\n",
            "Epoch:   2 | Train loss: 1.293 | Val loss: 1.391 | Gen: eway arway ontingsay-ingway isway onssay\n",
            "Epoch:   3 | Train loss: 1.073 | Val loss: 1.276 | Gen: ehay arway ondininghay isway onresway\n",
            "Epoch:   4 | Train loss: 0.919 | Val loss: 1.088 | Gen: ehay away onindioncay isway oonray\n",
            "Epoch:   5 | Train loss: 0.782 | Val loss: 1.041 | Gen: ehay iway ondincincedway isway orkingway\n",
            "Epoch:   6 | Train loss: 0.687 | Val loss: 1.006 | Gen: ehay iway ondingingsbay isway orknigsway\n",
            "Epoch:   7 | Train loss: 0.599 | Val loss: 0.876 | Gen: ehay airway ondinciontray isway orkway\n",
            "Epoch:   8 | Train loss: 0.514 | Val loss: 0.830 | Gen: ehay airway ondincitionway isway orkingway\n",
            "Epoch:   9 | Train loss: 0.452 | Val loss: 0.822 | Gen: ethay aitway ondiciontiongway isway okingsray\n",
            "Epoch:  10 | Train loss: 0.407 | Val loss: 0.788 | Gen: ethay airway ondincitionway isway okwray\n",
            "Epoch:  11 | Train loss: 0.380 | Val loss: 0.803 | Gen: ethay airway onditionceray isway okensryway\n",
            "Epoch:  12 | Train loss: 0.346 | Val loss: 0.776 | Gen: ethay airway ontioncitray isway ovedray\n",
            "Epoch:  13 | Train loss: 0.334 | Val loss: 0.728 | Gen: ehtway airway ondintrioncay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.284 | Val loss: 0.695 | Gen: ethay airway onditincedray isway orkway\n",
            "Epoch:  15 | Train loss: 0.243 | Val loss: 0.672 | Gen: ethay airway ondincitiongray isway orkingway\n",
            "Epoch:  16 | Train loss: 0.205 | Val loss: 0.679 | Gen: ehtay airway ondintiblepay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.178 | Val loss: 0.617 | Gen: ethay airway ondintibligpay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.158 | Val loss: 0.617 | Gen: ethay airway ondintiblecay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.144 | Val loss: 0.633 | Gen: ethay airway ondintiontray isway orkingway\n",
            "Epoch:  20 | Train loss: 0.142 | Val loss: 0.661 | Gen: ethay airway ondintibleday isway orkingway\n",
            "Epoch:  21 | Train loss: 0.148 | Val loss: 0.693 | Gen: ethay airway ondintibatqueray isway orkingway\n",
            "Epoch:  22 | Train loss: 0.154 | Val loss: 0.656 | Gen: ethay airway onditinciblay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.154 | Val loss: 0.719 | Gen: ethay airway onditionpray isway orkingway\n",
            "Epoch:  24 | Train loss: 0.160 | Val loss: 0.733 | Gen: ethay airway ondintionteray isway orkingway\n",
            "Epoch:  25 | Train loss: 0.157 | Val loss: 0.723 | Gen: ethay airway ondidingromay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.130 | Val loss: 0.645 | Gen: ethay airway ondinitioncay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.097 | Val loss: 0.585 | Gen: ethay airway ondidingicableway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.074 | Val loss: 0.563 | Gen: ethay airway onditinoncigway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.062 | Val loss: 0.561 | Gen: ethay airway onditinoncigway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.054 | Val loss: 0.554 | Gen: ethay airway onditinoncigway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.048 | Val loss: 0.561 | Gen: ethay airway onditinoncigway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.043 | Val loss: 0.564 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.039 | Val loss: 0.568 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.036 | Val loss: 0.570 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.033 | Val loss: 0.576 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.032 | Val loss: 0.580 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  37 | Train loss: 0.030 | Val loss: 0.579 | Gen: ethay airway ondintiousecay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.027 | Val loss: 0.569 | Gen: ethay airway onditionnigcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.026 | Val loss: 0.583 | Gen: ethay airway onditinonceway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.025 | Val loss: 0.625 | Gen: ethay airway onintibodingway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.027 | Val loss: 0.672 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.056 | Val loss: 0.793 | Gen: ethay airway onindiontveway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.152 | Val loss: 0.898 | Gen: ethay airway ondingicibay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.207 | Val loss: 0.869 | Gen: ethay airway ontiblindatmay isway oringflay\n",
            "Epoch:  45 | Train loss: 0.180 | Val loss: 0.705 | Gen: ethay airway onditingouray isway orkingway\n",
            "Epoch:  46 | Train loss: 0.113 | Val loss: 0.678 | Gen: ethay airway ondintibacy isway orkingway\n",
            "Epoch:  47 | Train loss: 0.066 | Val loss: 0.641 | Gen: ethay airway onditiningchay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.043 | Val loss: 0.648 | Gen: ethay airway onditiningchay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.032 | Val loss: 0.648 | Gen: ethay airway onditionmingay isway orkingway\n",
            "Obtained lowest validation loss of: 0.5536855818215936\n",
            "training time: 172.10703169699997 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionmingay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Q6zBT0b16ALe",
        "outputId": "0ab84cbf-6bbc-41b7-9097-3117d0ae55d0"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':64,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "\n",
        "import time\n",
        "t1 = time.process_time()\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "print(f\"training time: {time.process_time() - t1} \")\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.141 | Val loss: 1.943 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay intay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.695 | Val loss: 1.795 | Gen: elay-ay-ay-ay-ay-ay- away-ay onsay-onsay-away ilway-ay-ay-ay-ay-ay onay-ay-ay-ay-ay-ay-\n",
            "Epoch:   2 | Train loss: 1.459 | Val loss: 1.614 | Gen: etay-ay away-ay onternay-antay-ay iay-ay oodgay-ay-ay\n",
            "Epoch:   3 | Train loss: 1.253 | Val loss: 1.491 | Gen: ateray away-ay ondingnay-andway isway ormay-ingway\n",
            "Epoch:   4 | Train loss: 1.088 | Val loss: 1.378 | Gen: ateray away-awlay onsingnay-inway isway ormay-ingway\n",
            "Epoch:   5 | Train loss: 0.920 | Val loss: 1.152 | Gen: ehay away-inway ondingingsay isway odgay-ingway\n",
            "Epoch:   6 | Train loss: 0.784 | Val loss: 1.091 | Gen: etay away ondingingsay isway orgingway\n",
            "Epoch:   7 | Train loss: 0.696 | Val loss: 1.049 | Gen: ehay away ondingingsay isway odigeray\n",
            "Epoch:   8 | Train loss: 0.621 | Val loss: 1.011 | Gen: ehay away ondinglay-intecay isway orgay-ingway\n",
            "Epoch:   9 | Train loss: 0.553 | Val loss: 0.851 | Gen: ethay away ondigingtay isway orgingway\n",
            "Epoch:  10 | Train loss: 0.482 | Val loss: 0.858 | Gen: ethay airway ondigintioncay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.444 | Val loss: 0.808 | Gen: ethay ariway ondigintay-ancay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.406 | Val loss: 0.778 | Gen: ethay airway ondigintednay isway oriknay\n",
            "Epoch:  13 | Train loss: 0.384 | Val loss: 0.759 | Gen: ethay airway ondingintingray isway orkingway\n",
            "Epoch:  14 | Train loss: 0.338 | Val loss: 0.703 | Gen: ethay ariway ondigintioncay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.303 | Val loss: 0.668 | Gen: ethay ariway ondingintionway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.275 | Val loss: 0.691 | Gen: ethay ariway ondigningsray isway orknay-ingway\n",
            "Epoch:  17 | Train loss: 0.272 | Val loss: 0.700 | Gen: ethay airway ondingingtay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.260 | Val loss: 0.643 | Gen: ethay airway ondigintioncay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.240 | Val loss: 0.628 | Gen: ethay airway ondignitingway isway orkingway\n",
            "Epoch:  20 | Train loss: 0.219 | Val loss: 0.652 | Gen: ethay airway ondigtinedcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.213 | Val loss: 0.591 | Gen: ethay airway onditingnay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.194 | Val loss: 0.619 | Gen: ethay airway onditinginocay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.198 | Val loss: 0.581 | Gen: ethay airway onditingneday isway orkingway\n",
            "Epoch:  24 | Train loss: 0.184 | Val loss: 0.576 | Gen: ethay airway onditingenay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.171 | Val loss: 0.584 | Gen: ethay airway onditingenay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.167 | Val loss: 0.553 | Gen: ethay airway onditingnay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.160 | Val loss: 0.556 | Gen: ethay airway onditingeray isway orkingway\n",
            "Epoch:  28 | Train loss: 0.135 | Val loss: 0.496 | Gen: ethay airway onditingnay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.118 | Val loss: 0.512 | Gen: ethay airway onditioncingay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.106 | Val loss: 0.487 | Gen: ethay airway onditiondcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.096 | Val loss: 0.472 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.089 | Val loss: 0.476 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.084 | Val loss: 0.473 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.079 | Val loss: 0.471 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.076 | Val loss: 0.472 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.073 | Val loss: 0.496 | Gen: ethay airway onditionnicgay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.082 | Val loss: 0.819 | Gen: ethay airway onditionmingay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.174 | Val loss: 0.984 | Gen: ethay airway ondingitonalphay isway orknay-ingway\n",
            "Epoch:  39 | Train loss: 0.300 | Val loss: 0.791 | Gen: ethay airway ondignetionsay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.257 | Val loss: 0.669 | Gen: ethay airway ondingitinglay isway orkinway-angway\n",
            "Epoch:  41 | Train loss: 0.190 | Val loss: 0.556 | Gen: ethay airway ondigintioncay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.130 | Val loss: 0.483 | Gen: ethay airway ondigintioncay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.099 | Val loss: 0.469 | Gen: ethay airway ondigintioncay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.084 | Val loss: 0.459 | Gen: ethay airway ondigitnercay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.075 | Val loss: 0.457 | Gen: ethay airway onditingercay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.069 | Val loss: 0.454 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.064 | Val loss: 0.446 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.059 | Val loss: 0.450 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.056 | Val loss: 0.442 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.44241685340047004\n",
            "training time: 217.246730114 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "c62c1e77-07df-4f76-e7ba-79c4dcf84b36"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd1b69d7-4d96-4d40-bc9d-61685af5e4bf"
      },
      "source": [
        "best_encoder = rnn_encode_l # Replace with rnn_losses_s or rnn_losses l\n",
        "best_decoder = rnn_decoder_l # etc.\n",
        "best_args = rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = 'i want to get an a plus in this fascinating course'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\ti want to get an a plus in this fascinating course \n",
            "translated:\tiway antway otay etgay anway away usplay inway isthay ascinatisgfay ourescay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        #expand_size only works if size of corresponding dimension is 1\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys) #B x 1 x H -> B x S x H\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2) #B x S x 2H, so encoder and decoder hidden size are the same? --yes as they come from the same set of alphabets\n",
        "        unnormalized_attention = self.attention_network(concat_inputs) #B x S x H -> B x S x 1\n",
        "        attention_weights = self.softmax(unnormalized_attention) #B x S x 1\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values) #B x 1 x S; B x 1 x H\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"       \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # (D + H), H, H-> batch_size x hidden_size            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7cf40314-c93d-4206-f904-19311f64d2e8"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "rnn_attn_args = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':50, \r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':0.005,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'early_stopping_patience':10,\r\n",
        "              'batch_size':64, \r\n",
        "              'hidden_size':32, \r\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\r\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\r\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\r\n",
        "}\r\n",
        "rnn_attn_args.update(args_dict)\r\n",
        "\r\n",
        "print_opts(rnn_attn_args)\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "t1 = time.process_time()\r\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\r\n",
        "print(f\"training time:{time.process_time() - t1} \")\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.338 | Val loss: 1.992 | Gen: ay inay insay-ay-ay-ay-ay inway insay-ay\n",
            "Epoch:   1 | Train loss: 1.830 | Val loss: 1.766 | Gen: ateday atay inllay-ingsay indway inssay-ingay\n",
            "Epoch:   2 | Train loss: 1.586 | Val loss: 1.571 | Gen: eday arway ingsingsay-ingay isway onsssay-ay-ay\n",
            "Epoch:   3 | Train loss: 1.397 | Val loss: 1.525 | Gen: ededway alway ingncingseday isway onssedway\n",
            "Epoch:   4 | Train loss: 1.267 | Val loss: 1.356 | Gen: ehtay atay oncionstingway isway orconway\n",
            "Epoch:   5 | Train loss: 1.141 | Val loss: 1.406 | Gen: etway atinway oncontingsway isway ontsingway\n",
            "Epoch:   6 | Train loss: 1.048 | Val loss: 1.203 | Gen: ethay away-ay oncotingtingway isway ondtionday\n",
            "Epoch:   7 | Train loss: 0.948 | Val loss: 1.250 | Gen: ethay aray ontioniondway isway oringsway\n",
            "Epoch:   8 | Train loss: 0.864 | Val loss: 1.138 | Gen: ethay arway ongtingingsionway isway ortnhay-ingway\n",
            "Epoch:   9 | Train loss: 0.789 | Val loss: 1.028 | Gen: ehtay ariway onitingingstay isway oringsway\n",
            "Epoch:  10 | Train loss: 0.711 | Val loss: 0.970 | Gen: ehtay airay onitinonthay isway oringmay\n",
            "Epoch:  11 | Train loss: 0.643 | Val loss: 1.028 | Gen: ethay airway ontiousiontway isway ortingway\n",
            "Epoch:  12 | Train loss: 0.602 | Val loss: 0.976 | Gen: ehtway ariway ontionsionway isway oringsway\n",
            "Epoch:  13 | Train loss: 0.591 | Val loss: 1.002 | Gen: ehtway ariway ontioustingway isway ortingway\n",
            "Epoch:  14 | Train loss: 0.546 | Val loss: 0.857 | Gen: ethay airay oncitinontay isway oringmay\n",
            "Epoch:  15 | Train loss: 0.469 | Val loss: 0.871 | Gen: ehtway airway oncioningingway isway oringmay\n",
            "Epoch:  16 | Train loss: 0.450 | Val loss: 0.791 | Gen: ehtay airway ontioningionway isway oringmay\n",
            "Epoch:  17 | Train loss: 0.404 | Val loss: 1.010 | Gen: ehay airway onway-iointionday isway oringhay\n",
            "Epoch:  18 | Train loss: 0.426 | Val loss: 0.805 | Gen: ethay airway onitionsionway isway oringsway\n",
            "Epoch:  19 | Train loss: 0.376 | Val loss: 0.713 | Gen: ehtway airway ondiciontingway isway orcingway\n",
            "Epoch:  20 | Train loss: 0.339 | Val loss: 0.726 | Gen: ehtway airway onditiontingway isway oringhway\n",
            "Epoch:  21 | Train loss: 0.303 | Val loss: 0.771 | Gen: ethay airway ondioitingsay isway oringpay\n",
            "Epoch:  22 | Train loss: 0.280 | Val loss: 0.655 | Gen: ethay airway onditioingplay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.246 | Val loss: 0.690 | Gen: ethay airway onwiointingshay isway oringcay\n",
            "Epoch:  24 | Train loss: 0.232 | Val loss: 0.650 | Gen: ethay airway ondiitionshay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.222 | Val loss: 0.805 | Gen: ethay airway ondioitingpay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.280 | Val loss: 0.682 | Gen: ethay airway onditinonginway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.238 | Val loss: 0.644 | Gen: ethay airway ondiitionshay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.201 | Val loss: 0.900 | Gen: ethay airway onditingpingay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.227 | Val loss: 0.736 | Gen: ethay airway onditinontingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.218 | Val loss: 0.817 | Gen: ethay airway ingitionsingray isway orkingway\n",
            "Epoch:  31 | Train loss: 0.217 | Val loss: 0.615 | Gen: ethay airway onditiontingway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.171 | Val loss: 0.556 | Gen: ethay airway onditionicenday isway orkingway\n",
            "Epoch:  33 | Train loss: 0.153 | Val loss: 0.528 | Gen: ethay airway onditionishay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.121 | Val loss: 0.490 | Gen: ethay airway onditioningpay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.101 | Val loss: 0.517 | Gen: ethay airway onditiontingway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.094 | Val loss: 0.477 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.090 | Val loss: 0.680 | Gen: ethay airway ondiitiondioway isway orkingway\n",
            "Epoch:  38 | Train loss: 0.144 | Val loss: 0.555 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.111 | Val loss: 0.536 | Gen: ethay airway onditionicngay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.108 | Val loss: 0.625 | Gen: ethay airway onditiontingway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.111 | Val loss: 0.548 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.098 | Val loss: 0.559 | Gen: ethay airway onditioningmay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.114 | Val loss: 0.660 | Gen: ehtay airway onwitionsingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.119 | Val loss: 0.624 | Gen: ethay airway ondtionitninway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.104 | Val loss: 0.494 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.077 | Val loss: 0.496 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.47737857570783016\n",
            "training time:210.19463381599996 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionplinway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e9dd1d6-452a-4f49-f467-fa3a9a8b1f50"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionplinway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CVfYswoP_TEr",
        "outputId": "2b9fe285-e9ca-408e-b5ec-4d2c800e58bd"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512, \n",
        "              'hidden_size':32, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "\n",
        "import time\n",
        "\n",
        "t1 = time.process_time()\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "print(f\"training time:{time.process_time() - t1} \")\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.279 | Val loss: 2.082 | Gen: inay inay otay-ay-ay-ay inay onay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.863 | Val loss: 1.886 | Gen: ay-ay inay otintay-intay-ay inay otay-intay\n",
            "Epoch:   2 | Train loss: 1.680 | Val loss: 1.726 | Gen: ay-ay-ay ay-ay-ay otay-intay-otay-inwa ilway otay-intay\n",
            "Epoch:   3 | Train loss: 1.543 | Val loss: 1.648 | Gen: inway ay-away otintay-ontay-onway isway otay-ontay\n",
            "Epoch:   4 | Train loss: 1.426 | Val loss: 1.563 | Gen: atiway ay-inay ontinghentay-y ilway ontedsay\n",
            "Epoch:   5 | Train loss: 1.325 | Val loss: 1.506 | Gen: etay ay-inay onstiontenghay isway onstedway\n",
            "Epoch:   6 | Train loss: 1.230 | Val loss: 1.485 | Gen: etay ay-ilway ontiontiontay ilway ontionway\n",
            "Epoch:   7 | Train loss: 1.150 | Val loss: 1.414 | Gen: etay ay-iarway onsintentay-inay isway onstinway\n",
            "Epoch:   8 | Train loss: 1.067 | Val loss: 1.388 | Gen: ethay ay-awlay onsonstenglay isway onsonday\n",
            "Epoch:   9 | Train loss: 0.983 | Val loss: 1.312 | Gen: etay ainay onsentiontenay isway ormunsway\n",
            "Epoch:  10 | Train loss: 0.916 | Val loss: 1.249 | Gen: ethay aidway onstionstenay isway orningway\n",
            "Epoch:  11 | Train loss: 0.839 | Val loss: 1.226 | Gen: etay aray ovinestiontay isway orginghay\n",
            "Epoch:  12 | Train loss: 0.787 | Val loss: 1.082 | Gen: etay ariway onstinonstenay isway orunsiway\n",
            "Epoch:  13 | Train loss: 0.751 | Val loss: 1.060 | Gen: ethay ainway onstinestinway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.683 | Val loss: 1.005 | Gen: etsay aidway onstingingclay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.617 | Val loss: 0.970 | Gen: ethay airway ontinistingray isway orkingway\n",
            "Epoch:  16 | Train loss: 0.545 | Val loss: 0.921 | Gen: ethay airway onsiogingoncay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.488 | Val loss: 0.932 | Gen: ethay airway ontiningingcay isway oungisway\n",
            "Epoch:  18 | Train loss: 0.457 | Val loss: 0.920 | Gen: ethay airway onsigiongionway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.434 | Val loss: 0.804 | Gen: ethay airway onditingingcay isway orkngbray\n",
            "Epoch:  20 | Train loss: 0.387 | Val loss: 0.811 | Gen: thday airway ovitingnengcay isway orkngray\n",
            "Epoch:  21 | Train loss: 0.347 | Val loss: 0.668 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.327 | Val loss: 0.718 | Gen: ethay airway ontistingincay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.306 | Val loss: 0.867 | Gen: ethay airway ondingingingway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.321 | Val loss: 0.694 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.281 | Val loss: 0.622 | Gen: ethay airway onditioningcay isway orkngglay\n",
            "Epoch:  26 | Train loss: 0.245 | Val loss: 0.602 | Gen: thay airway onditiongingway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.217 | Val loss: 0.525 | Gen: tehay airway onditiongincay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.200 | Val loss: 0.508 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.190 | Val loss: 0.479 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.158 | Val loss: 0.444 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.141 | Val loss: 0.510 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.142 | Val loss: 0.621 | Gen: ethay airway onditingicgcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.145 | Val loss: 0.440 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.105 | Val loss: 0.354 | Gen: tthay airway onditiongincay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.094 | Val loss: 0.415 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.096 | Val loss: 0.435 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.094 | Val loss: 0.350 | Gen: tthay airway onditiongincay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.083 | Val loss: 0.969 | Gen: tthay airway ondiongingchay isway orkggemay\n",
            "Epoch:  39 | Train loss: 0.191 | Val loss: 0.551 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.117 | Val loss: 0.398 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.092 | Val loss: 0.419 | Gen: ethay airway onditionicgcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.094 | Val loss: 0.471 | Gen: ethay airway onditiouningway isway orkingsway\n",
            "Epoch:  43 | Train loss: 0.106 | Val loss: 0.419 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.098 | Val loss: 0.391 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.113 | Val loss: 0.388 | Gen: tekay airway onditiongingay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.078 | Val loss: 0.253 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.056 | Val loss: 0.255 | Gen: tekay airway onditiongingway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.043 | Val loss: 0.246 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.032 | Val loss: 0.237 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Obtained lowest validation loss of: 0.23746878130664556\n",
            "training time:267.17376460899993 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongincay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_VnH1S3V_p36",
        "outputId": "a9772bf9-93b9-4696-c3f8-f2711b34214a"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "\n",
        "import time\n",
        "\n",
        "t1 = time.process_time()\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "print(f\"training time:{time.process_time() - t1} \")\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.102 | Val loss: 1.832 | Gen: ontay illlay illlay-intiontay isay illlay\n",
            "Epoch:   1 | Train loss: 1.558 | Val loss: 1.565 | Gen: itiongway aray oontioncoontay-ay isay oorsay-ongay\n",
            "Epoch:   2 | Train loss: 1.236 | Val loss: 1.304 | Gen: ethay away ontioncay-omomay isway orsingway\n",
            "Epoch:   3 | Train loss: 1.000 | Val loss: 1.199 | Gen: ethay arway ontionconcouway isway orway-ingway\n",
            "Epoch:   4 | Train loss: 0.831 | Val loss: 1.126 | Gen: ethay arway onctay-oncationway isway orway-ingway\n",
            "Epoch:   5 | Train loss: 0.700 | Val loss: 0.956 | Gen: ethay away ontioncay-ompay-oway isway orkingway\n",
            "Epoch:   6 | Train loss: 0.574 | Val loss: 0.813 | Gen: ethay airway oncictiongday isway orkingway\n",
            "Epoch:   7 | Train loss: 0.452 | Val loss: 0.708 | Gen: ehay airway onditiondingway isway orkingway\n",
            "Epoch:   8 | Train loss: 0.358 | Val loss: 0.675 | Gen: ehtway airway ondintiningway isway orkingsay\n",
            "Epoch:   9 | Train loss: 0.304 | Val loss: 0.672 | Gen: ehay airway onditingingway isway orkingway\n",
            "Epoch:  10 | Train loss: 0.274 | Val loss: 0.598 | Gen: ehay airway onditiongingway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.233 | Val loss: 0.632 | Gen: ethay airway ondinionionday isway orkingway\n",
            "Epoch:  12 | Train loss: 0.209 | Val loss: 0.701 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.169 | Val loss: 0.524 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.126 | Val loss: 0.433 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.096 | Val loss: 0.379 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.070 | Val loss: 0.417 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.060 | Val loss: 0.327 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.048 | Val loss: 0.383 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.052 | Val loss: 0.496 | Gen: ethay airway onditiongicnay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.086 | Val loss: 0.571 | Gen: ethay airway onditingingway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.175 | Val loss: 0.630 | Gen: ethay airway onctioncoitionway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.183 | Val loss: 0.488 | Gen: eheay airway onditiondingway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.116 | Val loss: 0.428 | Gen: ehtay airway ondintioningway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.079 | Val loss: 0.399 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.045 | Val loss: 0.337 | Gen: ethay airway onditionincay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.031 | Val loss: 0.377 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.027 | Val loss: 0.292 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.017 | Val loss: 0.275 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.012 | Val loss: 0.266 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.010 | Val loss: 0.264 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.008 | Val loss: 0.260 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.007 | Val loss: 0.257 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.006 | Val loss: 0.255 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.006 | Val loss: 0.253 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.005 | Val loss: 0.252 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.005 | Val loss: 0.251 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.004 | Val loss: 0.251 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.004 | Val loss: 0.250 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.004 | Val loss: 0.250 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.003 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.003 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.003 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.003 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.003 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.002 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.002 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.002 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.002 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.002 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.248209192176416\n",
            "training time:213.20427971200002 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NAT3bfRO_ubt",
        "outputId": "e5dace2d-841f-4e98-ce53-8766278fe314"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "\n",
        "import time\n",
        "\n",
        "t1 = time.process_time()\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "print(f\"training time:{time.process_time() - t1} \")\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.073 | Val loss: 1.851 | Gen: otay-ay away-ay ontintay-ay-ay-ay-ay istay ontay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.556 | Val loss: 1.597 | Gen: etay away-ay ontiontiontay-ay isway ontay-ay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.272 | Val loss: 1.446 | Gen: etay away-ay ontingay-ingay-ingay isway ongray-ingray\n",
            "Epoch:   3 | Train loss: 1.068 | Val loss: 1.321 | Gen: etay away-ybay ontay-intay-inway isway ondway-inway\n",
            "Epoch:   4 | Train loss: 0.927 | Val loss: 1.209 | Gen: etay away-inway ontay-inionday-inway isway ongay-ingsay\n",
            "Epoch:   5 | Train loss: 0.787 | Val loss: 1.123 | Gen: etay away-ybay ontay-ininationway isway orgray-ay\n",
            "Epoch:   6 | Train loss: 0.694 | Val loss: 1.034 | Gen: etay arway ontay-intionachay isway orway-ighnay\n",
            "Epoch:   7 | Train loss: 0.588 | Val loss: 0.926 | Gen: etay arway ondiontifingay isway orway-ingway\n",
            "Epoch:   8 | Train loss: 0.514 | Val loss: 0.902 | Gen: etay ay-inway ondionicinay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.454 | Val loss: 0.829 | Gen: etay arway ondingingcay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.408 | Val loss: 0.704 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.329 | Val loss: 0.654 | Gen: ehay airway onditioncingay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.289 | Val loss: 0.668 | Gen: ehtay airway onditingsnay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.236 | Val loss: 0.539 | Gen: ehtay airway onditingingcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.204 | Val loss: 0.539 | Gen: ehtay airway onditiongingay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.174 | Val loss: 0.452 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.145 | Val loss: 0.391 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.117 | Val loss: 0.483 | Gen: ethay airway onditiconingway isway orkway-ingway\n",
            "Epoch:  18 | Train loss: 0.117 | Val loss: 0.474 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.173 | Val loss: 0.608 | Gen: ehtay airway onditititionsay isway orkginway\n",
            "Epoch:  20 | Train loss: 0.149 | Val loss: 0.426 | Gen: ethay airway ondititinglay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.099 | Val loss: 0.430 | Gen: ethay airway ondititionckay isway owkingway\n",
            "Epoch:  22 | Train loss: 0.096 | Val loss: 0.391 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.077 | Val loss: 0.349 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.056 | Val loss: 0.405 | Gen: ethay airway ondititionglay isway orkginway\n",
            "Epoch:  25 | Train loss: 0.058 | Val loss: 0.329 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.055 | Val loss: 0.657 | Gen: ethay ariwway ondintiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.113 | Val loss: 0.361 | Gen: ethay airway ondioningingay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.068 | Val loss: 0.328 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.050 | Val loss: 0.293 | Gen: ethay airway ondititionncay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.045 | Val loss: 0.257 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.032 | Val loss: 0.183 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.020 | Val loss: 0.167 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.015 | Val loss: 0.160 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.012 | Val loss: 0.157 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.010 | Val loss: 0.150 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.009 | Val loss: 0.147 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.008 | Val loss: 0.146 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.007 | Val loss: 0.145 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.006 | Val loss: 0.143 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.006 | Val loss: 0.142 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.005 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.005 | Val loss: 0.139 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.004 | Val loss: 0.139 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.004 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.004 | Val loss: 0.139 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.003 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.003 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.003 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.003 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.13690966166410912\n",
            "training time:266.6082599749998 \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = queries.shape[0]\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size) #B x k x H\n",
        "        k = self.K(keys) #B x S x H\n",
        "        v = self.V(values) #B x S x H\n",
        "        unnormalized_attention = k.bmm(q.transpose(1,2)) * self.scaling_factor # B x S x H & B x H x k -> B x S x k\n",
        "        attention_weights = self.softmax(unnormalized_attention) #B x S x k\n",
        "        context = attention_weights.transpose(1,2).bmm(v) #B x k x S & B x S x H -> B x k x H\n",
        "        return context, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TlWpVsAQxXp-",
        "outputId": "9684378d-5c31-496e-cf49-adb734222823"
      },
      "source": [
        "A = torch.tensor([[[1,2],[3,4],[9,10]], [[5,6],[7,8],[11,12]]])\n",
        "neg_inf = torch.tensor(-1e7)\n",
        "A = A + neg_inf.expand_as(A).tril(diagonal = -1) \n",
        "A"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.0000e+00,  2.0000e+00],\n",
              "         [-1.0000e+07,  4.0000e+00],\n",
              "         [-1.0000e+07, -1.0000e+07]],\n",
              "\n",
              "        [[ 5.0000e+00,  6.0000e+00],\n",
              "         [-1.0000e+07,  8.0000e+00],\n",
              "         [-1.0000e+07, -1.0000e+07]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = queries.shape[0]\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size) #B x k x H\n",
        "        k = self.K(keys) #B x S x H\n",
        "        v = self.V(values) #B x S x H\n",
        "        unnormalized_attention = k.bmm(q.transpose(1,2)) * self.scaling_factor \n",
        "        unnormalized_attention = unnormalized_attention + self.neg_inf.expand_as(unnormalized_attention).tril(diagonal=-1).to(device ='cuda') #B x S x k\n",
        "        attention_weights = self.softmax(unnormalized_attention) #B x S x k\n",
        "        context = attention_weights.transpose(1,2).bmm(v) #B x k x S & B x S x H -> B x k x H\n",
        "        return context, attention_weights"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "          #Why input 3 annotations to self_attentions[i]?\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "976583cd-f983-4f22-cddf-619348bc1e7f"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans32_args_s = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':100, \r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 100,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 64,\r\n",
        "              'hidden_size': 32,\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans32_args_s.update(args_dict)\r\n",
        "print_opts(trans32_args_s)\r\n",
        "\r\n",
        "import time \r\n",
        "t1 = time.process_time()\r\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\r\n",
        "print(f\"training time: {time.process_time() - t1}\")\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.900 | Val loss: 2.408 | Gen: ay inay ingingaynginngingayn ay inginay\n",
            "Epoch:   1 | Train loss: 2.152 | Val loss: 2.085 | Gen: eway iay ongingay ieay ongingay\n",
            "Epoch:   2 | Train loss: 1.895 | Val loss: 1.925 | Gen: way iay ongingay ieway oongay\n",
            "Epoch:   3 | Train loss: 1.728 | Val loss: 1.859 | Gen: way iay ongingay iway oiogay\n",
            "Epoch:   4 | Train loss: 1.600 | Val loss: 1.743 | Gen: way iay ontiongay isway oiongay\n",
            "Epoch:   5 | Train loss: 1.496 | Val loss: 1.743 | Gen: way iay oniongday iway oiongay\n",
            "Epoch:   6 | Train loss: 1.423 | Val loss: 1.810 | Gen: way iay ooniongionoooooooooo iway ooogiogay\n",
            "Epoch:   7 | Train loss: 1.341 | Val loss: 1.608 | Gen: way aiririay oniongiontionongoooo iway oooogingiongy\n",
            "Epoch:   8 | Train loss: 1.264 | Val loss: 1.525 | Gen: way iray onindingingonay isway ooongingiongy\n",
            "Epoch:   9 | Train loss: 1.195 | Val loss: 1.459 | Gen: way iray odindingintiongongoo isway oingway\n",
            "Epoch:  10 | Train loss: 1.136 | Val loss: 1.417 | Gen: way iray odindingintiongway isway oingenay\n",
            "Epoch:  11 | Train loss: 1.100 | Val loss: 1.409 | Gen: way irayway odindingintenongway isway oingway\n",
            "Epoch:  12 | Train loss: 1.082 | Val loss: 1.468 | Gen: way iray-ay odingray-incay isway okokokingway\n",
            "Epoch:  13 | Train loss: 1.041 | Val loss: 1.397 | Gen: way iray oindintintingway isway oinghingway\n",
            "Epoch:  14 | Train loss: 0.983 | Val loss: 1.273 | Gen: way iray odingratintincay isway oinghingway\n",
            "Epoch:  15 | Train loss: 0.923 | Val loss: 1.272 | Gen: ehway iray odingintintiongway isway oinghingway\n",
            "Epoch:  16 | Train loss: 0.889 | Val loss: 1.275 | Gen: ehway iray odingratintincongway isway oingenghay\n",
            "Epoch:  17 | Train loss: 0.857 | Val loss: 1.286 | Gen: ehway iray odingintintiongway isway oinghenay\n",
            "Epoch:  18 | Train loss: 0.827 | Val loss: 1.268 | Gen: ehway iray odingray isway oingenay\n",
            "Epoch:  19 | Train loss: 0.812 | Val loss: 1.269 | Gen: ehway iray ondiongrintincay isway oingenay\n",
            "Epoch:  20 | Train loss: 0.782 | Val loss: 1.224 | Gen: ehay iray ondioningenay isway oingenghay\n",
            "Epoch:  21 | Train loss: 0.755 | Val loss: 1.231 | Gen: ehway irayway odincingentincay isway oikingenay\n",
            "Epoch:  22 | Train loss: 0.742 | Val loss: 1.170 | Gen: ehay iray odinciongdenay isway oikokingway\n",
            "Epoch:  23 | Train loss: 0.739 | Val loss: 1.215 | Gen: ehay iray odingentintiongway isway oingenay\n",
            "Epoch:  24 | Train loss: 0.688 | Val loss: 1.161 | Gen: ehay irayway oonidintingway isway oikingway\n",
            "Epoch:  25 | Train loss: 0.660 | Val loss: 1.300 | Gen: ehay iray onoidiongway isway oikokiongway\n",
            "Epoch:  26 | Train loss: 0.692 | Val loss: 1.219 | Gen: ehay iray-ayway ondionientingway isway okokingenay\n",
            "Epoch:  27 | Train loss: 0.632 | Val loss: 1.111 | Gen: ehay iray ondioniingcay isway okokingway\n",
            "Epoch:  28 | Train loss: 0.600 | Val loss: 1.128 | Gen: ehay iray ondioningcancontiowa isway okingnay\n",
            "Epoch:  29 | Train loss: 0.573 | Val loss: 1.138 | Gen: ehay iray onidioingcay isway oikingway\n",
            "Epoch:  30 | Train loss: 0.553 | Val loss: 1.065 | Gen: ehay iray ondiidingcay isway oikingnay\n",
            "Epoch:  31 | Train loss: 0.534 | Val loss: 1.053 | Gen: ehay iray ondiiingcinioway isway oikingenay\n",
            "Epoch:  32 | Train loss: 0.505 | Val loss: 1.044 | Gen: ehay iray ondiiongcincay isway oikingnay\n",
            "Epoch:  33 | Train loss: 0.486 | Val loss: 1.026 | Gen: ehay iray ondiiiningcay isway oikingnay\n",
            "Epoch:  34 | Train loss: 0.475 | Val loss: 1.010 | Gen: ehay iray ondiiingcingway isway oikingnay\n",
            "Epoch:  35 | Train loss: 0.462 | Val loss: 1.057 | Gen: ehay iray ondidiningcay isway oikingway\n",
            "Epoch:  36 | Train loss: 0.456 | Val loss: 1.031 | Gen: ehay iray ondidiniingway isway okingenay\n",
            "Epoch:  37 | Train loss: 0.438 | Val loss: 1.006 | Gen: ehay iray ondiidingcanioway isway oikingnay\n",
            "Epoch:  38 | Train loss: 0.449 | Val loss: 1.077 | Gen: ehay arirway ondiinitingway isway okingkay\n",
            "Epoch:  39 | Train loss: 0.470 | Val loss: 1.150 | Gen: ehay airrway ondiditingcay isway owakingkgway\n",
            "Epoch:  40 | Train loss: 0.538 | Val loss: 1.242 | Gen: ehehay iray odindininingcantionc isway oikingnay\n",
            "Epoch:  41 | Train loss: 0.541 | Val loss: 0.931 | Gen: ehthay iray onditiingcay isway okikingway\n",
            "Epoch:  42 | Train loss: 0.421 | Val loss: 0.925 | Gen: ethay arirway ondititingcay isway oikingway\n",
            "Epoch:  43 | Train loss: 0.378 | Val loss: 0.913 | Gen: ethay arirway ondititingcay isway okingnay\n",
            "Epoch:  44 | Train loss: 0.359 | Val loss: 0.912 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  45 | Train loss: 0.346 | Val loss: 0.910 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  46 | Train loss: 0.335 | Val loss: 0.908 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  47 | Train loss: 0.325 | Val loss: 0.909 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  48 | Train loss: 0.316 | Val loss: 0.906 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  49 | Train loss: 0.308 | Val loss: 0.913 | Gen: ehthay arirway ondititingcay isway okingnay\n",
            "Epoch:  50 | Train loss: 0.301 | Val loss: 0.911 | Gen: ehthay arirway ondititingcay isway owkingnay\n",
            "Epoch:  51 | Train loss: 0.293 | Val loss: 0.920 | Gen: ehthay arirway ondititingcay isway owkingnay\n",
            "Epoch:  52 | Train loss: 0.286 | Val loss: 0.924 | Gen: ehthay arirway ondititingcay isway orkingnay\n",
            "Epoch:  53 | Train loss: 0.279 | Val loss: 0.953 | Gen: ehthay arirway ondititingcay isway orkingnay\n",
            "Epoch:  54 | Train loss: 0.279 | Val loss: 0.971 | Gen: ehthay arirway ondititingcay isway orkingnay\n",
            "Epoch:  55 | Train loss: 0.283 | Val loss: 1.049 | Gen: ehthay arirway ondititingcay isway orkingnay\n",
            "Epoch:  56 | Train loss: 0.283 | Val loss: 1.082 | Gen: ehthay arirway onditiningcay isway orkingnway\n",
            "Epoch:  57 | Train loss: 0.294 | Val loss: 1.046 | Gen: ehthay irray ondititingcay isway orkingnay\n",
            "Epoch:  58 | Train loss: 0.284 | Val loss: 0.969 | Gen: ehthay arirway onditinitingcay isway orkingnay\n",
            "Epoch:  59 | Train loss: 0.280 | Val loss: 1.108 | Gen: ehthay irray onditingcay isway okingnay\n",
            "Epoch:  60 | Train loss: 0.299 | Val loss: 0.903 | Gen: ehthay arirway ondititingca isway okingnay\n",
            "Epoch:  61 | Train loss: 0.281 | Val loss: 0.929 | Gen: ehthay ariway onditingcay isway owingenay\n",
            "Epoch:  62 | Train loss: 0.260 | Val loss: 0.959 | Gen: ehthay arirway onditingcay isway orkingnay\n",
            "Epoch:  63 | Train loss: 0.242 | Val loss: 0.929 | Gen: ehthay arirway onditiningcay isway oringkay\n",
            "Epoch:  64 | Train loss: 0.228 | Val loss: 0.894 | Gen: ehthay arirway onditiningcay isway orkingnway\n",
            "Epoch:  65 | Train loss: 0.219 | Val loss: 0.942 | Gen: ehthay arirway onditingcay isway orkingnay\n",
            "Epoch:  66 | Train loss: 0.213 | Val loss: 0.932 | Gen: ehthay arirway onditiningcay isway orkingnway\n",
            "Epoch:  67 | Train loss: 0.206 | Val loss: 0.938 | Gen: ehthay arirway onditingcay isway orkingnway\n",
            "Epoch:  68 | Train loss: 0.200 | Val loss: 0.945 | Gen: ehthay arirway onditingcay isway owkingnway\n",
            "Epoch:  69 | Train loss: 0.194 | Val loss: 0.954 | Gen: ehthay arirway onditiningcay isway orkingnway\n",
            "Epoch:  70 | Train loss: 0.190 | Val loss: 0.958 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  71 | Train loss: 0.184 | Val loss: 0.960 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  72 | Train loss: 0.180 | Val loss: 0.956 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  73 | Train loss: 0.175 | Val loss: 0.953 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  74 | Train loss: 0.171 | Val loss: 0.954 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  75 | Train loss: 0.166 | Val loss: 0.965 | Gen: ehthay arirway onditiningcay isway oikingnway\n",
            "Epoch:  76 | Train loss: 0.163 | Val loss: 0.969 | Gen: ehthay arirway onditiningcay isway owkingnway\n",
            "Epoch:  77 | Train loss: 0.159 | Val loss: 0.970 | Gen: ehthay arirway onditiningcay isway oikingnway\n",
            "Epoch:  78 | Train loss: 0.156 | Val loss: 1.028 | Gen: ehthay arirway onditinitingway isway orikingway\n",
            "Epoch:  79 | Train loss: 0.257 | Val loss: 1.290 | Gen: ehehway ariwaway onditiningcay isway oringngway\n",
            "Epoch:  80 | Train loss: 0.314 | Val loss: 1.034 | Gen: ehthay arirway ondiditingcay isway owikingway\n",
            "Epoch:  81 | Train loss: 0.239 | Val loss: 0.988 | Gen: ethay irray onditiningcay isway oikingnay\n",
            "Epoch:  82 | Train loss: 0.182 | Val loss: 0.912 | Gen: ehthay arirway ondititingcay isway oikingnway\n",
            "Epoch:  83 | Train loss: 0.161 | Val loss: 0.935 | Gen: ehthay arirway onditinitingcay isway oikingnway\n",
            "Epoch:  84 | Train loss: 0.153 | Val loss: 0.931 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  85 | Train loss: 0.147 | Val loss: 0.931 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  86 | Train loss: 0.142 | Val loss: 0.932 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  87 | Train loss: 0.137 | Val loss: 0.926 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  88 | Train loss: 0.133 | Val loss: 0.928 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  89 | Train loss: 0.130 | Val loss: 0.927 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  90 | Train loss: 0.126 | Val loss: 0.930 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  91 | Train loss: 0.123 | Val loss: 0.933 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  92 | Train loss: 0.120 | Val loss: 0.937 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  93 | Train loss: 0.117 | Val loss: 0.941 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  94 | Train loss: 0.115 | Val loss: 0.946 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  95 | Train loss: 0.112 | Val loss: 0.950 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  96 | Train loss: 0.111 | Val loss: 0.983 | Gen: ethay arirway onditiingcay isway oikingnway\n",
            "Epoch:  97 | Train loss: 0.111 | Val loss: 0.960 | Gen: ehthay arirway onditinitingcay isway oikingnway\n",
            "Epoch:  98 | Train loss: 0.107 | Val loss: 0.962 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Epoch:  99 | Train loss: 0.103 | Val loss: 0.964 | Gen: ehthay arirway onditiingcay isway oikingnway\n",
            "Obtained lowest validation loss of: 0.8941980186694612\n",
            "training time: 161.96384792800018\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay arirway onditiingcay isway oikingnway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1b808cab-bdb6-426c-a7e3-04afe76d5fe4"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay arirway onditiingcay isway oikingnway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a5e661db-f928-4899-c7ff-74026ac3587e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans32_args_l = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':100,\r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 10,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 512,\r\n",
        "              'hidden_size': 32,\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans32_args_l.update(args_dict)\r\n",
        "print_opts(trans32_args_l)\r\n",
        "\r\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.924 | Val loss: 2.537 | Gen: a-a-ay inayay inltinlayayltyyyyy inlinlinlayltayyy onaywayayayaywayay\n",
            "Epoch:   1 | Train loss: 2.233 | Val loss: 2.254 | Gen: a-a-ay-a-a-a-t a-ay ononlinlaylay-ayylay inlinlinay onayway\n",
            "Epoch:   2 | Train loss: 1.969 | Val loss: 2.086 | Gen: eattay-a-a-a-ay away-ay onglingionglinglay isway ongayway\n",
            "Epoch:   3 | Train loss: 1.784 | Val loss: 1.983 | Gen: atay-ay awayway onglinginginglay isway ongay-ay-ay-ay\n",
            "Epoch:   4 | Train loss: 1.653 | Val loss: 1.929 | Gen: tetay-ay awaywayayayay onglinginginginay isway ongay-ay-ay-ay\n",
            "Epoch:   5 | Train loss: 1.559 | Val loss: 1.839 | Gen: tay-ay awayway onglintingingintay isway ongingaywayway\n",
            "Epoch:   6 | Train loss: 1.475 | Val loss: 1.764 | Gen: tetay arawayay onglinginginginghay isay ogingay-ay\n",
            "Epoch:   7 | Train loss: 1.407 | Val loss: 1.740 | Gen: tetay-ay arayray ontintindonginway isway ogingay-ay\n",
            "Epoch:   8 | Train loss: 1.346 | Val loss: 1.634 | Gen: tetay arayray onginginginginway isway ooginay-ay\n",
            "Epoch:   9 | Train loss: 1.263 | Val loss: 1.557 | Gen: tay arayray onginginginay isay ogingay-ananwlay\n",
            "Epoch:  10 | Train loss: 1.213 | Val loss: 1.535 | Gen: tetay iaray ongingindatinday isway oogingway-ay\n",
            "Epoch:  11 | Train loss: 1.165 | Val loss: 1.459 | Gen: tay aray ongingingingnghay isway ogingway-ay-ingnay\n",
            "Epoch:  12 | Train loss: 1.104 | Val loss: 1.510 | Gen: tetay irayray ongingndingndinway isway ogsgsenay-ingnay\n",
            "Epoch:  13 | Train loss: 1.063 | Val loss: 1.438 | Gen: tetay araway ongingingingnghay isway ogingngnay-angnay\n",
            "Epoch:  14 | Train loss: 1.017 | Val loss: 1.497 | Gen: tetay irayray ongindindingndndhtin isway ogingngnay-ingnay\n",
            "Epoch:  15 | Train loss: 0.988 | Val loss: 1.346 | Gen: etay aray ongindindingndnday isway ogingnay-angnay\n",
            "Epoch:  16 | Train loss: 0.938 | Val loss: 1.360 | Gen: ethay airaway ongindindingndndnway isway ogingngnay\n",
            "Epoch:  17 | Train loss: 0.906 | Val loss: 1.241 | Gen: ethay aray ongindindingndnday isway ognay-ingnay\n",
            "Epoch:  18 | Train loss: 0.878 | Val loss: 1.223 | Gen: ethay airay ondndindingndindnway isway okgrangngsway\n",
            "Epoch:  19 | Train loss: 0.846 | Val loss: 1.173 | Gen: etay aray ondingindingnday isway okingray\n",
            "Epoch:  20 | Train loss: 0.837 | Val loss: 1.238 | Gen: ethhay airaway ondndindindindway isway okgrangngsway\n",
            "Epoch:  21 | Train loss: 0.800 | Val loss: 1.194 | Gen: etay araway ondindindingnday isway owanginay\n",
            "Epoch:  22 | Train loss: 0.775 | Val loss: 1.240 | Gen: ethay airaway ondndindndindndway isway owangnay\n",
            "Epoch:  23 | Train loss: 0.748 | Val loss: 1.264 | Gen: ethay araway ondndindingnay isway oagingnay\n",
            "Epoch:  24 | Train loss: 0.714 | Val loss: 1.240 | Gen: ethay airaway ondndindindindway isway okagray-onay\n",
            "Epoch:  25 | Train loss: 0.733 | Val loss: 1.389 | Gen: tetayhay airaaiaway ondinginginay isway okingray\n",
            "Epoch:  26 | Train loss: 0.773 | Val loss: 1.265 | Gen: ethay araway ondindindindidway isway okegray-onay\n",
            "Epoch:  27 | Train loss: 0.744 | Val loss: 1.240 | Gen: ethay airaway ondindindingnay isway okagray\n",
            "Epoch:  28 | Train loss: 0.664 | Val loss: 1.102 | Gen: ethay airaway ondindindinginay isway okingway\n",
            "Epoch:  29 | Train loss: 0.623 | Val loss: 1.073 | Gen: ethay airaway ondindindignay isway okingray\n",
            "Epoch:  30 | Train loss: 0.602 | Val loss: 1.052 | Gen: ethay airaway ondindindignay isway okingray\n",
            "Epoch:  31 | Train loss: 0.588 | Val loss: 1.026 | Gen: ethay airaway ondindigninay isway okingray\n",
            "Epoch:  32 | Train loss: 0.573 | Val loss: 1.014 | Gen: ethay airaway ondiditinginay isway okingray\n",
            "Epoch:  33 | Train loss: 0.555 | Val loss: 1.000 | Gen: ethay airaway ondingitionindigiay isway okingray\n",
            "Epoch:  34 | Train loss: 0.544 | Val loss: 1.018 | Gen: ethay airaway ondidictionginay isway okingray\n",
            "Epoch:  35 | Train loss: 0.536 | Val loss: 0.963 | Gen: ethay airaway onditingionindicay isway okingray\n",
            "Epoch:  36 | Train loss: 0.513 | Val loss: 0.943 | Gen: ethay airaway ondiditinginay isway okingray\n",
            "Epoch:  37 | Train loss: 0.498 | Val loss: 0.932 | Gen: ethay airaway onditingionindicay isway okingray\n",
            "Epoch:  38 | Train loss: 0.486 | Val loss: 0.935 | Gen: ethay airaway ondiditinginay isway okingray\n",
            "Epoch:  39 | Train loss: 0.472 | Val loss: 0.917 | Gen: ethay airaway onditionginicay isway okingray\n",
            "Epoch:  40 | Train loss: 0.461 | Val loss: 0.927 | Gen: ethay airaway onditiongicay isway okingray\n",
            "Epoch:  41 | Train loss: 0.454 | Val loss: 0.902 | Gen: ethay airaway onditingioningicay isway okingray\n",
            "Epoch:  42 | Train loss: 0.445 | Val loss: 0.878 | Gen: ethay airway onditionginicay isway okingray\n",
            "Epoch:  43 | Train loss: 0.433 | Val loss: 0.911 | Gen: ethay airaway onditingicay-inininc isway okingray\n",
            "Epoch:  44 | Train loss: 0.433 | Val loss: 0.880 | Gen: ethay airway onditiciway isway okrkngway\n",
            "Epoch:  45 | Train loss: 0.455 | Val loss: 0.844 | Gen: ethay airaway onditiongininay isway okingray\n",
            "Epoch:  46 | Train loss: 0.408 | Val loss: 0.799 | Gen: ethay airaway onditininginay isway okingray\n",
            "Epoch:  47 | Train loss: 0.388 | Val loss: 0.813 | Gen: ethay airway onditinicay isway okingray\n",
            "Epoch:  48 | Train loss: 0.373 | Val loss: 0.788 | Gen: ethay airway onditingionicay isway okrkingway\n",
            "Epoch:  49 | Train loss: 0.364 | Val loss: 0.771 | Gen: ethay airway onditionginicay isway okingray\n",
            "Epoch:  50 | Train loss: 0.356 | Val loss: 0.750 | Gen: ethay airway onditingioncay isway okrkngway\n",
            "Epoch:  51 | Train loss: 0.347 | Val loss: 0.743 | Gen: ethay airway onditinicay isway okingray\n",
            "Epoch:  52 | Train loss: 0.339 | Val loss: 0.726 | Gen: ethay airway onditinicay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.329 | Val loss: 0.723 | Gen: ethay airway onditionginicay isway okrkngway\n",
            "Epoch:  54 | Train loss: 0.321 | Val loss: 0.712 | Gen: ethay airway onditionginicay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.313 | Val loss: 0.705 | Gen: ethay airway onditionginicay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.305 | Val loss: 0.703 | Gen: ethay airway onditionginicay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.298 | Val loss: 0.694 | Gen: ethay airway onditionginicay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.291 | Val loss: 0.694 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.284 | Val loss: 0.692 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.278 | Val loss: 0.695 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.272 | Val loss: 0.686 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.265 | Val loss: 0.686 | Gen: ethay airway onditinicay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.259 | Val loss: 0.684 | Gen: ethay airway onditinicay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.254 | Val loss: 0.685 | Gen: ethay airway onditinicay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.248 | Val loss: 0.683 | Gen: ethay airway onditiongnicay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.242 | Val loss: 0.685 | Gen: ethay airway onditinicay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.237 | Val loss: 0.686 | Gen: ethay airway onditinicongncay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.233 | Val loss: 0.699 | Gen: ethay airway onditinicongncay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.229 | Val loss: 0.703 | Gen: ethay airway onditinicongncay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.225 | Val loss: 0.720 | Gen: ethay airway onditiningncay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.222 | Val loss: 0.724 | Gen: ethay airway onditiniongncay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.220 | Val loss: 0.699 | Gen: ethay airway onditiningncay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.238 | Val loss: 0.854 | Gen: ethay airway onditiongnicay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.398 | Val loss: 1.012 | Gen: ethay airway ondintiningcay isway orgingnkay\n",
            "Epoch:  75 | Train loss: 0.406 | Val loss: 0.772 | Gen: ethay airway onditiongnincay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.354 | Val loss: 0.682 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.261 | Val loss: 0.584 | Gen: ethay airway onditioncongicay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.223 | Val loss: 0.553 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.206 | Val loss: 0.547 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.199 | Val loss: 0.544 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.193 | Val loss: 0.542 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.188 | Val loss: 0.541 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.183 | Val loss: 0.540 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.179 | Val loss: 0.538 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.176 | Val loss: 0.538 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.173 | Val loss: 0.537 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.169 | Val loss: 0.537 | Gen: ethay airway onditioningticay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.166 | Val loss: 0.536 | Gen: ethay airway onditiniongticay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.164 | Val loss: 0.537 | Gen: ethay airway onditiniongticay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.161 | Val loss: 0.536 | Gen: ethay airway onditiniongticay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.158 | Val loss: 0.537 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  92 | Train loss: 0.155 | Val loss: 0.537 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  93 | Train loss: 0.153 | Val loss: 0.537 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  94 | Train loss: 0.150 | Val loss: 0.537 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  95 | Train loss: 0.147 | Val loss: 0.537 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  96 | Train loss: 0.145 | Val loss: 0.539 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  97 | Train loss: 0.142 | Val loss: 0.538 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Epoch:  98 | Train loss: 0.140 | Val loss: 0.540 | Gen: ethay airway onditinicongway isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.536214440226344\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditinicongway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "830affd5-9e38-41a8-dacc-4631962cc8ce"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('doubts', 'oubtsday')\n",
            "('increasing', 'increasingway')\n",
            "('public', 'ublicpay')\n",
            "('tallest', 'allesttay')\n",
            "('mere', 'eremay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.469 | Val loss: 1.903 | Gen: ay ay oy y ooooooooooooooooy\n",
            "Epoch:   1 | Train loss: 1.686 | Val loss: 1.665 | Gen: ay iwawawawawawawawawaw oooooooooooooooooooo iswaswasiay oooooooooooooooooooo\n",
            "Epoch:   2 | Train loss: 1.460 | Val loss: 1.580 | Gen:  awawawawawawawawawaa oooooooooooooooooooo issassaasay oooooooooooooooooooo\n",
            "Epoch:   3 | Train loss: 1.287 | Val loss: 1.435 | Gen: ay awawawawawawawawawaw oooooooooooooooooooo i oooooooooooooooooooo\n",
            "Epoch:   4 | Train loss: 1.134 | Val loss: 1.507 | Gen:  i oooooooooooooooooooo i oooayy\n",
            "Epoch:   5 | Train loss: 0.996 | Val loss: 1.201 | Gen: ehhhhahahaahay i ingigingigiiniinngni i wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:   6 | Train loss: 0.826 | Val loss: 1.170 | Gen:  i oooooooooooooooooooo i oooooooooooooooooooo\n",
            "Epoch:   7 | Train loss: 0.742 | Val loss: 1.068 | Gen:  arrrrrrrrrrrrrrraaar oooooooooooooooooooo i wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:   8 | Train loss: 0.625 | Val loss: 0.991 | Gen:  i  i               \n",
            "Epoch:   9 | Train loss: 0.520 | Val loss: 0.895 | Gen:   incicicicy i wwwwwwwwwaayyyyyyEOSy\n",
            "Epoch:  10 | Train loss: 0.410 | Val loss: 0.679 | Gen:   oooooooooooooooooooo  \n",
            "Epoch:  11 | Train loss: 0.350 | Val loss: 0.625 | Gen:  aaaaaaa oooooooooooooooooooo  wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  12 | Train loss: 0.274 | Val loss: 0.586 | Gen:  aaaaaaaaaaaaaaaaaaaa oooooooooooooooooooo  wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  13 | Train loss: 0.211 | Val loss: 0.495 | Gen: yyEOSEOSEOShayyEOSyy aaaaaaaaaaaaaaaaaaaa inccccccyy  wwwwwwwway\n",
            "Epoch:  14 | Train loss: 0.159 | Val loss: 0.460 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  15 | Train loss: 0.149 | Val loss: 0.501 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  16 | Train loss: 0.133 | Val loss: 0.449 | Gen:     wwwwwwwwwwwwaway\n",
            "Epoch:  17 | Train loss: 0.091 | Val loss: 0.450 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  18 | Train loss: 0.090 | Val loss: 0.393 | Gen:   oooooooooooooooooooo  wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  19 | Train loss: 0.067 | Val loss: 0.330 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  20 | Train loss: 0.049 | Val loss: 0.305 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  21 | Train loss: 0.036 | Val loss: 0.389 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  22 | Train loss: 0.119 | Val loss: 1.062 | Gen: hhhhhhhhhhhhhhhhhhhh  oooooooooooooooooooo isiieiiiiiwwwwwaawwi wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  23 | Train loss: 0.246 | Val loss: 0.585 | Gen:                     \n",
            "Epoch:  24 | Train loss: 0.192 | Val loss: 0.716 | Gen:  aaaaaaaaaaaaaaaaaaaa   wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  25 | Train loss: 0.127 | Val loss: 0.295 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  26 | Train loss: 0.055 | Val loss: 0.219 | Gen:                     \n",
            "Epoch:  27 | Train loss: 0.023 | Val loss: 0.221 | Gen:                     \n",
            "Epoch:  28 | Train loss: 0.015 | Val loss: 0.206 | Gen:                     \n",
            "Epoch:  29 | Train loss: 0.010 | Val loss: 0.208 | Gen:                     \n",
            "Epoch:  30 | Train loss: 0.008 | Val loss: 0.206 | Gen:                     \n",
            "Epoch:  31 | Train loss: 0.007 | Val loss: 0.208 | Gen:                     \n",
            "Epoch:  32 | Train loss: 0.005 | Val loss: 0.210 | Gen:                     \n",
            "Epoch:  33 | Train loss: 0.005 | Val loss: 0.211 | Gen:                     \n",
            "Epoch:  34 | Train loss: 0.005 | Val loss: 0.210 | Gen:                     \n",
            "Epoch:  35 | Train loss: 0.004 | Val loss: 0.215 | Gen:                     \n",
            "Epoch:  36 | Train loss: 0.006 | Val loss: 0.237 | Gen:                     \n",
            "Epoch:  37 | Train loss: 0.013 | Val loss: 0.237 | Gen:     wwwwwwwwwwwwwwwwwwww\n",
            "Epoch:  38 | Train loss: 0.009 | Val loss: 0.242 | Gen:                     \n",
            "Epoch:  39 | Train loss: 0.017 | Val loss: 0.346 | Gen:                     \n",
            "Epoch:  40 | Train loss: 0.063 | Val loss: 1.045 | Gen:                     \n",
            "Epoch:  41 | Train loss: 0.209 | Val loss: 0.327 | Gen:                     \n",
            "Epoch:  42 | Train loss: 0.065 | Val loss: 0.267 | Gen:                     \n",
            "Epoch:  43 | Train loss: 0.046 | Val loss: 0.312 | Gen:                     \n",
            "Epoch:  44 | Train loss: 0.043 | Val loss: 0.238 | Gen:                     \n",
            "Epoch:  45 | Train loss: 0.027 | Val loss: 0.256 | Gen:                     \n",
            "Epoch:  46 | Train loss: 0.056 | Val loss: 0.190 | Gen:                     \n",
            "Epoch:  47 | Train loss: 0.028 | Val loss: 0.240 | Gen:                     \n",
            "Epoch:  48 | Train loss: 0.020 | Val loss: 0.192 | Gen:                     \n",
            "Epoch:  49 | Train loss: 0.010 | Val loss: 0.181 | Gen:                     \n",
            "Obtained lowest validation loss of: 0.18129198537644697\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\t    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "034f5bf1-d081-48ba-c931-17a91228168e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans64_args_l = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':50,\r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 20,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 512, \r\n",
        "              'hidden_size': 64, # Increased model size\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans64_args_l.update(args_dict)\r\n",
        "print_opts(trans64_args_l)\r\n",
        "\r\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('rts', 'rtsay')\n",
            "('cash', 'ashcay')\n",
            "('emergence', 'emergenceway')\n",
            "('thunderbird', 'underbirdthay')\n",
            "('arbor', 'arborway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.283 | Val loss: 1.948 | Gen: way-tay-tway away onongay-onngay way oray-igay-y-ay-ay\n",
            "Epoch:   1 | Train loss: 1.603 | Val loss: 1.668 | Gen: etay awaray ongay-inay-inay way oray-igay\n",
            "Epoch:   2 | Train loss: 1.378 | Val loss: 1.582 | Gen: etay-hway-ay-ay-ay-a away ontinay iway-ay odway-igway\n",
            "Epoch:   3 | Train loss: 1.230 | Val loss: 1.469 | Gen: ethay-ay-ay-ay irarayray oniongatinay-inatinw isway oray-inghay-igway\n",
            "Epoch:   4 | Train loss: 1.095 | Val loss: 1.564 | Gen: ehtay-heway-ay iray-iiraway ontingingngay-ingay isway orgggggggggay-ngway\n",
            "Epoch:   5 | Train loss: 0.988 | Val loss: 1.347 | Gen: etay iray oninginginginay iway oway-ingway\n",
            "Epoch:   6 | Train loss: 0.867 | Val loss: 1.193 | Gen: ethay ay ointiniginay isway oway-ingway\n",
            "Epoch:   7 | Train loss: 0.751 | Val loss: 1.160 | Gen: etay iray ondinigay-iningay iway oway-ingway\n",
            "Epoch:   8 | Train loss: 0.653 | Val loss: 1.093 | Gen: ethay arirway oniditiningnay isway oway-ingway\n",
            "Epoch:   9 | Train loss: 0.605 | Val loss: 1.114 | Gen: etay airway onditininingay isway oway-igway\n",
            "Epoch:  10 | Train loss: 0.548 | Val loss: 0.917 | Gen: etay ariway ondidingingay isway orkay-ingway\n",
            "Epoch:  11 | Train loss: 0.473 | Val loss: 0.990 | Gen: ethay arway onditiningay iway oway-ingway\n",
            "Epoch:  12 | Train loss: 0.428 | Val loss: 0.910 | Gen: ethay ariay ondiningcay isway oway-ingway\n",
            "Epoch:  13 | Train loss: 0.416 | Val loss: 0.843 | Gen: ethay airway onidiningcay isway oway-ingway\n",
            "Epoch:  14 | Train loss: 0.362 | Val loss: 0.905 | Gen: ethay ariay onditincingngway isway oway-ingway\n",
            "Epoch:  15 | Train loss: 0.342 | Val loss: 0.825 | Gen: ethay airway onidiningcay isway oworkay\n",
            "Epoch:  16 | Train loss: 0.318 | Val loss: 0.709 | Gen: ethay airway onditiniongcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.279 | Val loss: 0.732 | Gen: ethay aiay ondicincay isway owrkingway\n",
            "Epoch:  18 | Train loss: 0.283 | Val loss: 0.875 | Gen: ethay iarway onditiningcay issway owkingway\n",
            "Epoch:  19 | Train loss: 0.429 | Val loss: 0.893 | Gen: ethay arway ocindinioningay iway owrkingway\n",
            "Epoch:  20 | Train loss: 0.383 | Val loss: 0.716 | Gen: ethay airway onditigningway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.268 | Val loss: 0.623 | Gen: ethay airway onditingingcay-ionay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.199 | Val loss: 0.512 | Gen: ethay airway onditingingcay isway owrkingway\n",
            "Epoch:  23 | Train loss: 0.169 | Val loss: 0.484 | Gen: ethay airway onditingcingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.138 | Val loss: 0.471 | Gen: ethay airway onditingingcay isway owrkingway\n",
            "Epoch:  25 | Train loss: 0.126 | Val loss: 0.505 | Gen: ethay airway onditingingcay iswway orkingway\n",
            "Epoch:  26 | Train loss: 0.127 | Val loss: 0.464 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.114 | Val loss: 0.475 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.105 | Val loss: 0.442 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.122 | Val loss: 0.514 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.110 | Val loss: 0.550 | Gen: edthay airway onditiongingcay isway owkingway\n",
            "Epoch:  31 | Train loss: 0.162 | Val loss: 0.764 | Gen: ethay airway onditionigcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.174 | Val loss: 0.799 | Gen: etethay aiaway onditiongncay issway orkingway\n",
            "Epoch:  33 | Train loss: 0.213 | Val loss: 0.577 | Gen: ethay aiaway onditioningncay iaway owrkingway\n",
            "Epoch:  34 | Train loss: 0.118 | Val loss: 0.496 | Gen: ethay airway onditionigcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.078 | Val loss: 0.423 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.063 | Val loss: 0.351 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.049 | Val loss: 0.353 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.044 | Val loss: 0.357 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.040 | Val loss: 0.363 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.042 | Val loss: 0.360 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.034 | Val loss: 0.368 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.033 | Val loss: 0.445 | Gen: ethay airway onditioningcay issway orkingway\n",
            "Epoch:  43 | Train loss: 0.043 | Val loss: 0.434 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.077 | Val loss: 0.520 | Gen: ethay airway onditiongincay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.167 | Val loss: 1.145 | Gen: ethay aiway ondigingingway isway owayday\n",
            "Epoch:  46 | Train loss: 0.234 | Val loss: 0.554 | Gen: ethay airway onditiongngcay isway owrkingway\n",
            "Epoch:  47 | Train loss: 0.110 | Val loss: 0.596 | Gen: ethay airway onditioningcay isway oxingway\n",
            "Epoch:  48 | Train loss: 0.082 | Val loss: 0.330 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.040 | Val loss: 0.332 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.32990003904362897\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "3fb51356-9ea2-4644-94ab-a1603e98cfa9"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\r\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "f98e67fd-8e50-44fa-cda2-021f2447fe8b"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "d0bd6432-fda7-409d-a7dd-60ac5f694f65"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, trans64_args_l, )"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-160d02230264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_WORD_ATTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'street'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_WORD_ATTN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans64_args_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFuYSpiKGZq1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}